"""
YOLOv8 ëª¨ë¸ Test Dataset í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ (3 Class)

## ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”?
í›ˆë ¨ëœ YOLOv8 ëª¨ë¸ì„ Test Datasetìœ¼ë¡œ ìµœì¢… í‰ê°€í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
Validation setì´ ì•„ë‹Œ Test setìœ¼ë¡œ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ê²€ì¦í•©ë‹ˆë‹¤.

## íƒì§€ í´ë˜ìŠ¤
- Class 0: helmet (í—¬ë©§ ì°©ìš©) âœ…
- Class 1: head (í—¬ë©§ ë¯¸ì°©ìš©) âš ï¸
- Class 2: vest (ì•ˆì „ì¡°ë¼ ì°©ìš©) âœ…

## ì‚¬ìš© ë°©ë²•
```bash
# ê¸°ë³¸ ì‹¤í–‰ (best.pt ëª¨ë¸, test dataset)
uv run python src/4_test/evaluate_test.py

# íŠ¹ì • ëª¨ë¸ ì§€ì •
uv run python src/4_test/evaluate_test.py --model models/ppe_detection/weights/last.pt

# Confidence threshold ë³€ê²½
uv run python src/4_test/evaluate_test.py --conf 0.25

# IoU threshold ë³€ê²½
uv run python src/4_test/evaluate_test.py --iou 0.6
```

## í‰ê°€ í•­ëª©
1. mAP@0.5, mAP@0.5:0.95
2. Precision, Recall
3. Confusion Matrix
4. í´ë˜ìŠ¤ë³„ AP (Average Precision)
5. Validation vs Test ì„±ëŠ¥ ë¹„êµ

## ì¶œë ¥ íŒŒì¼
í‰ê°€ ì™„ë£Œ í›„ ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:
- test_results/confusion_matrix.png: Test set í˜¼ë™ í–‰ë ¬
- test_results/BoxPR_curve.png: Precision-Recall ê³¡ì„ 
- test_results/results.csv: ìƒì„¸ í‰ê°€ ê²°ê³¼
- test_report.md: ìµœì¢… í‰ê°€ ë³´ê³ ì„œ
"""

import argparse
import os
from pathlib import Path
import yaml
from ultralytics import YOLO
import pandas as pd
from datetime import datetime


def load_validation_results(model_dir):
    """
    Validation set ê²°ê³¼ ë¡œë“œ (ë¹„êµìš©)

    Args:
        model_dir: ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ

    Returns:
        dict: Validation ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    results_csv = Path(model_dir) / 'results.csv'

    if not results_csv.exists():
        print("âš ï¸ Validation ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ë§ˆì§€ë§‰ epochì˜ ê²°ê³¼ ì½ê¸°
    df = pd.read_csv(results_csv)
    last_row = df.iloc[-1]

    return {
        'mAP50': last_row['metrics/mAP50(B)'],
        'mAP50-95': last_row['metrics/mAP50-95(B)'],
        'precision': last_row['metrics/precision(B)'],
        'recall': last_row['metrics/recall(B)']
    }


def evaluate_test_set(args):
    """
    Test Datasetìœ¼ë¡œ ëª¨ë¸ í‰ê°€ ì‹¤í–‰

    Args:
        args: ëª…ë ¹ì¤„ ì¸ì (argparse Namespace)

    ì‹¤í–‰ ìˆœì„œ:
    1. ëª¨ë¸ ë¡œë“œ
    2. Test set í‰ê°€
    3. ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
    4. Validation vs Test ë¹„êµ
    """

    # =========================================================================
    # 1. ì´ˆê¸°í™”
    # =========================================================================
    print("=" * 70)
    print("YOLOv8 ëª¨ë¸ Test Dataset í‰ê°€ (3 Class)")
    print("=" * 70)
    print()

    # ëª¨ë¸ ê²½ë¡œ í™•ì¸
    model_path = Path(args.model)
    if not model_path.exists():
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return

    # ë°ì´í„°ì…‹ ì„¤ì • íŒŒì¼ í™•ì¸
    data_yaml = Path(args.data)
    if not data_yaml.exists():
        print(f"âŒ ë°ì´í„°ì…‹ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_yaml}")
        return

    print(f"ğŸ“¦ ëª¨ë¸: {model_path}")
    print(f"ğŸ“„ ë°ì´í„°ì…‹: {data_yaml}")
    print(f"ğŸ¯ Confidence threshold: {args.conf}")
    print(f"ğŸ“ IoU threshold: {args.iou}")
    print()

    # =========================================================================
    # 2. ëª¨ë¸ ë¡œë“œ
    # =========================================================================
    print("ğŸ¤– YOLOv8 ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = YOLO(str(model_path))
    print("   âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    print()

    # =========================================================================
    # 3. Test Set í‰ê°€
    # =========================================================================
    print("ğŸ§ª Test Dataset í‰ê°€ ì‹œì‘...")
    print("-" * 70)

    # Test setìœ¼ë¡œ í‰ê°€ (split='test' ì§€ì •)
    results = model.val(
        data=str(data_yaml),
        split='test',  # test dataset ì‚¬ìš©
        conf=args.conf,
        iou=args.iou,
        batch=args.batch,
        save_json=True,  # COCO JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
        save_hybrid=True,  # í•˜ì´ë¸Œë¦¬ë“œ ë¼ë²¨ ì €ì¥
        plots=True,  # ê·¸ë˜í”„ ìƒì„±
        verbose=True
    )

    print()
    print("=" * 70)
    print("âœ… Test Dataset í‰ê°€ ì™„ë£Œ!")
    print("=" * 70)
    print()

    # =========================================================================
    # 4. ê²°ê³¼ ì¶œë ¥
    # =========================================================================
    print("ğŸ“Š Test Set ì„±ëŠ¥ ì§€í‘œ:")
    print("-" * 70)

    # ë©”íŠ¸ë¦­ ì¶”ì¶œ
    test_metrics = {
        'mAP50': results.box.map50,
        'mAP50-95': results.box.map,
        'precision': results.box.mp,
        'recall': results.box.mr
    }

    print(f"   ğŸ“ˆ mAP@0.5: {test_metrics['mAP50']:.4f} ({test_metrics['mAP50']*100:.2f}%)")
    print(f"   ğŸ“ˆ mAP@0.5:0.95: {test_metrics['mAP50-95']:.4f} ({test_metrics['mAP50-95']*100:.2f}%)")
    print(f"   ğŸ¯ Precision: {test_metrics['precision']:.4f} ({test_metrics['precision']*100:.2f}%)")
    print(f"   ğŸ” Recall: {test_metrics['recall']:.4f} ({test_metrics['recall']*100:.2f}%)")
    print()

    # í´ë˜ìŠ¤ë³„ AP
    if hasattr(results.box, 'ap_class_index'):
        print("ğŸ“Š í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
        print("-" * 70)
        class_names = ['helmet', 'head', 'vest']

        # ap50 ì¶”ì¶œ
        ap50_per_class = results.box.ap50

        for idx, class_name in enumerate(class_names):
            if idx < len(ap50_per_class):
                ap_value = ap50_per_class[idx]
                print(f"   {class_name:8s}: AP@0.5 = {ap_value:.4f} ({ap_value*100:.2f}%)")
        print()

    # =========================================================================
    # 5. Validation vs Test ë¹„êµ
    # =========================================================================
    model_dir = model_path.parent.parent
    val_metrics = load_validation_results(model_dir)

    if val_metrics:
        print("ğŸ“Š Validation vs Test ì„±ëŠ¥ ë¹„êµ:")
        print("-" * 70)
        print(f"{'ì§€í‘œ':<20} {'Validation':>15} {'Test':>15} {'ì°¨ì´':>15}")
        print("-" * 70)

        for metric_name in ['mAP50', 'mAP50-95', 'precision', 'recall']:
            val_value = val_metrics[metric_name]
            test_value = test_metrics[metric_name]
            diff = test_value - val_value
            diff_pct = (diff / val_value * 100) if val_value != 0 else 0

            print(f"{metric_name:<20} {val_value:>14.4f} {test_value:>14.4f} {diff:>+14.4f} ({diff_pct:+.2f}%)")

        print("-" * 70)
        print()

        # ê³¼ì í•© íŒë‹¨
        mAP_diff = test_metrics['mAP50'] - val_metrics['mAP50']
        if abs(mAP_diff) < 0.02:  # 2% ì´ë‚´ ì°¨ì´
            print("âœ… íŒì •: ì¼ë°˜í™” ì„±ëŠ¥ ìš°ìˆ˜ (Validation â‰ˆ Test)")
        elif mAP_diff < -0.05:  # 5% ì´ìƒ í•˜ë½
            print("âš ï¸ íŒì •: ê³¼ì í•© ê°€ëŠ¥ì„± ìˆìŒ (Test < Validation)")
        else:
            print("âœ… íŒì •: ì •ìƒ ë²”ìœ„")
        print()

    # =========================================================================
    # 6. ê²°ê³¼ íŒŒì¼ ì €ì¥
    # =========================================================================
    print("ğŸ’¾ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì¤‘...")

    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path('output/test_results')
    output_dir.mkdir(parents=True, exist_ok=True)

    # ê²°ê³¼ CSV ì €ì¥
    results_data = {
        'timestamp': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')],
        'model': [str(model_path)],
        'test_mAP50': [test_metrics['mAP50']],
        'test_mAP50-95': [test_metrics['mAP50-95']],
        'test_precision': [test_metrics['precision']],
        'test_recall': [test_metrics['recall']]
    }

    if val_metrics:
        results_data['val_mAP50'] = [val_metrics['mAP50']]
        results_data['val_mAP50-95'] = [val_metrics['mAP50-95']]
        results_data['val_precision'] = [val_metrics['precision']]
        results_data['val_recall'] = [val_metrics['recall']]

    df = pd.DataFrame(results_data)
    csv_path = output_dir / 'test_evaluation_results.csv'
    df.to_csv(csv_path, index=False)
    print(f"   âœ… CSV ì €ì¥: {csv_path}")

    # Validation ê²°ê³¼ì™€ í•¨ê»˜ ì‹œê°í™” íŒŒì¼ ì´ë™
    # YOLOv8ì´ ìë™ ìƒì„±í•œ íŒŒì¼ë“¤ì„ test_resultsë¡œ ì´ë™
    runs_dir = Path('runs/detect')
    if runs_dir.exists():
        latest_dir = max(runs_dir.glob('val*'), key=os.path.getctime, default=None)
        if latest_dir:
            import shutil
            for img_file in ['confusion_matrix.png', 'confusion_matrix_normalized.png',
                           'BoxPR_curve.png', 'BoxF1_curve.png',
                           'BoxP_curve.png', 'BoxR_curve.png']:
                src = latest_dir / img_file
                if src.exists():
                    dst = output_dir / f'test_{img_file}'
                    shutil.copy(src, dst)
                    print(f"   âœ… ì‹œê°í™” ì €ì¥: {dst}")

    print()
    print("=" * 70)
    print("ğŸ‰ ëª¨ë“  í‰ê°€ ì™„ë£Œ!")
    print("=" * 70)
    print()
    print("ğŸ“‚ ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜:")
    print(f"   ğŸ“ {output_dir.absolute()}")
    print()
    print("ë‹¤ìŒ ë‹¨ê³„:")
    print("   1ï¸âƒ£ test_results/ í´ë”ì—ì„œ ê²°ê³¼ í™•ì¸")
    print("   2ï¸âƒ£ Confusion Matrix ë¶„ì„")
    print("   3ï¸âƒ£ ìµœì¢… ë³´ê³ ì„œ ì‘ì„±")
    print()

    return results


def main():
    """
    ë©”ì¸ í•¨ìˆ˜ - ëª…ë ¹ì¤„ ì¸ì íŒŒì‹± ë° í‰ê°€ ì‹¤í–‰
    """
    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    base_dir = Path(__file__).parent.parent.parent

    default_model = base_dir / 'models' / 'ppe_detection' / 'weights' / 'best.pt'
    default_data = base_dir / 'configs' / 'ppe_dataset.yaml'

    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(
        description='YOLOv8 PPE Detection ëª¨ë¸ Test Dataset í‰ê°€',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument(
        '--model',
        type=str,
        default=str(default_model),
        help='í‰ê°€í•  ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (default: models/ppe_detection/weights/best.pt)'
    )

    parser.add_argument(
        '--data',
        type=str,
        default=str(default_data),
        help='ë°ì´í„°ì…‹ YAML íŒŒì¼ ê²½ë¡œ (default: configs/ppe_dataset.yaml)'
    )

    parser.add_argument(
        '--conf',
        type=float,
        default=0.001,
        help='Confidence threshold (default: 0.001 for evaluation)'
    )

    parser.add_argument(
        '--iou',
        type=float,
        default=0.6,
        help='IoU threshold for NMS (default: 0.6)'
    )

    parser.add_argument(
        '--batch',
        type=int,
        default=32,
        help='Batch size (default: 32)'
    )

    args = parser.parse_args()

    # í‰ê°€ ì‹¤í–‰
    evaluate_test_set(args)


if __name__ == '__main__':
    main()
