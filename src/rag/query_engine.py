"""
RAG Query Engine - LLM-based Answer Generation

Vector Search + LLM Generation for domain-specific QA
FAISS ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•˜ì—¬ OpenAI LLMì´ ë‹µë³€ ìƒì„±
"""

import os
from typing import List, Dict, Optional
from openai import OpenAI

from .vector_store import FAISSVectorStore


class RAGQueryEngine:
    """
    RAG (Retrieval-Augmented Generation) ì¿¼ë¦¬ ì—”ì§„

    1. Retrieval: FAISS ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    2. Augmentation: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    3. Generation: OpenAI LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±

    Attributes:
        vector_store (FAISSVectorStore): ë²¡í„° ì €ì¥ì†Œ
        client (OpenAI): OpenAI API í´ë¼ì´ì–¸íŠ¸
        model (str): LLM ëª¨ë¸ (gpt-4-turbo-preview ë˜ëŠ” gpt-3.5-turbo)
        temperature (float): LLM ì˜¨ë„ (0.0~1.0, ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì )
        max_tokens (int): ë‹µë³€ ìµœëŒ€ ê¸¸ì´
        top_k (int): ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜
    """

    def __init__(
        self,
        vector_store: FAISSVectorStore,
        model: str = "gpt-4-turbo-preview",
        temperature: float = 0.3,
        max_tokens: int = 500,
        top_k: int = 3,
        api_key: Optional[str] = None
    ):
        """
        RAG ì¿¼ë¦¬ ì—”ì§„ ì´ˆê¸°í™”

        Args:
            vector_store: FAISS ë²¡í„° ì €ì¥ì†Œ
            model: OpenAI LLM ëª¨ë¸ëª…
            temperature: LLM ì˜¨ë„ (ë‚®ì„ìˆ˜ë¡ ì‚¬ì‹¤ ê¸°ë°˜, ë†’ì„ìˆ˜ë¡ ì°½ì˜ì )
            max_tokens: ë‹µë³€ ìµœëŒ€ í† í° ìˆ˜
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜
            api_key: OpenAI API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ ë³€ìˆ˜)
        """
        self.vector_store = vector_store
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.top_k = top_k

    def _build_prompt(self, query: str, contexts: List[Dict]) -> str:
        """
        í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Few-shot + Context-aware)

        ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬í•¨í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìƒì„±

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            contexts: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

        Returns:
            str: LLM í”„ë¡¬í”„íŠ¸
        """
        # ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ í¬ë§·íŒ…
        # ê²€ìƒ‰ëœ Top-K ë¬¸ì„œë“¤ì„ ë²ˆí˜¸ì™€ ì¶œì²˜ì™€ í•¨ê»˜ ë‚˜ì—´
        # LLMì´ ì°¸ì¡°í•  ìˆ˜ ìˆë„ë¡ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì œê³µ
        context_text = "\n\n".join([
            f"[ë¬¸ì„œ {i+1}] (ì¶œì²˜: {ctx['metadata'].get('source', 'N/A')})\n{ctx['text']}"
            for i, ctx in enumerate(contexts)
        ])

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Few-shot + Domain-specific)
        # RAGì˜ í•µì‹¬: Retrievalëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µí•˜ì—¬ í™˜ê°(Hallucination) ë°©ì§€
        #
        # í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ëµ:
        # 1. Role-playing: "ì „ë™í‚¥ë³´ë“œ ì•ˆì „ êµìœ¡ ì „ë¬¸ê°€" ì—­í•  ë¶€ì—¬
        # 2. Context grounding: ì°¸ê³  ë¬¸ì„œë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì œê³µ
        # 3. Instruction: 5ê°€ì§€ ëª…í™•í•œ ì§€ì¹¨ ì œì‹œ
        # 4. Few-shot (ì•”ë¬µì ): ë‹µë³€ í˜•ì‹ ì˜ˆì‹œ ë‚´í¬
        # 5. Constraint: 2-3ë¬¸ì¥ ì œí•œìœ¼ë¡œ ê°„ê²°ì„± í™•ë³´
        prompt = f"""ë‹¹ì‹ ì€ ì „ë™í‚¥ë³´ë“œ ì•ˆì „ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì°¸ê³  ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

**ì°¸ê³  ë¬¸ì„œ:**
{context_text}

**ì¤‘ìš” ì§€ì¹¨:**
1. ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
2. ë²•ê·œëŠ” ì •í™•í•œ ì¡°í•­ê³¼ ë²Œê¸ˆì„ ëª…ì‹œí•˜ì„¸ìš”.
3. ì•ˆì „ ê°€ì´ë“œëŠ” êµ¬ì²´ì ì¸ ë°©ë²•ì„ ì„¤ëª…í•˜ì„¸ìš”.
4. ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ì œê³µëœ ìë£Œì—ëŠ” í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
5. ë‹µë³€ì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.

**ì§ˆë¬¸:** {query}

**ë‹µë³€:**"""

        return prompt

    def query(self, question: str, return_sources: bool = True) -> Dict[str, any]:
        """
        RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ

        1. FAISS ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (Retrieval)
        2. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Augmentation)
        3. OpenAI LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (Generation)

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            return_sources: ì¶œì²˜ ë¬¸ì„œ ë°˜í™˜ ì—¬ë¶€

        Returns:
            Dict: ë‹µë³€ ê²°ê³¼
                - answer (str): ìƒì„±ëœ ë‹µë³€
                - sources (List[Dict]): ì°¸ê³ í•œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (return_sources=Trueì¼ ë•Œ)
                - metadata (Dict): ë©”íƒ€ë°ì´í„° (ëª¨ë¸, ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ë“±)

        Example:
            >>> engine = RAGQueryEngine(vector_store)
            >>> result = engine.query("í—¬ë©§ ì•ˆ ì“°ë©´ ë²Œê¸ˆ ì–¼ë§ˆì•¼?")
            >>> print(result["answer"])
            "ë„ë¡œêµí†µë²• ì œ160ì¡°ì— ë”°ë¼ í—¬ë©§ ë¯¸ì°©ìš© ì‹œ ê³¼íƒœë£Œ 2ë§Œì›ì´ ë¶€ê³¼ë©ë‹ˆë‹¤."
        """
        # ========================================================================
        # 1. Retrieval: FAISS ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        # ========================================================================
        # - ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        # - FAISS IndexFlatL2ë¡œ L2 ê±°ë¦¬ ê¸°ë°˜ Top-K ê²€ìƒ‰
        # - ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì„œ Kê°œ ë°˜í™˜ (Semantic Search)
        search_results = self.vector_store.search(question, top_k=self.top_k)

        # ê²€ìƒ‰ ì‹¤íŒ¨ ì²˜ë¦¬ (ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ)
        if not search_results:
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ë¥´ê²Œ í‘œí˜„í•´ì£¼ì‹œê² ì–´ìš”?",
                "sources": [],
                "metadata": {
                    "model": self.model,
                    "num_sources": 0,
                    "search_success": False
                }
            }

        # ========================================================================
        # 2. Augmentation: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        # ========================================================================
        # - ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í”„ë¡¬í”„íŠ¸ì— ì‚½ì…
        # - LLMì´ ì°¸ì¡°í•  ìˆ˜ ìˆëŠ” ì§€ì‹ ì¦ê°• (Knowledge Augmentation)
        # - í™˜ê°(Hallucination) ë°©ì§€: ë¬¸ì„œì— ê¸°ë°˜í•œ ë‹µë³€ ìœ ë„
        prompt = self._build_prompt(question, search_results)

        # ========================================================================
        # 3. Generation: OpenAI LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        # ========================================================================
        # OpenAI Chat Completions API í˜¸ì¶œ
        # - model: gpt-4-turbo-preview ë˜ëŠ” gpt-3.5-turbo
        # - temperature: 0.3 (ë‚®ìŒ, ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€ì— ì í•©)
        #   * 0.0: ê²°ì •ë¡ ì  (í•­ìƒ ê°™ì€ ë‹µë³€)
        #   * 1.0: ì°½ì˜ì  (ë§¤ë²ˆ ë‹¤ë¥¸ ë‹µë³€)
        # - max_tokens: 500 (ë‹µë³€ ê¸¸ì´ ì œí•œ)
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                # System message: AIì˜ ì—­í•  ì •ì˜ (í˜ë¥´ì†Œë‚˜ ì„¤ì •)
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë™í‚¥ë³´ë“œ ì•ˆì „ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                # User message: ì‹¤ì œ í”„ë¡¬í”„íŠ¸ (ì§ˆë¬¸ + ì»¨í…ìŠ¤íŠ¸)
                {"role": "user", "content": prompt}
            ],
            temperature=self.temperature,  # ë‚®ì€ ì˜¨ë„ â†’ ì¼ê´€ì  ë‹µë³€
            max_tokens=self.max_tokens  # ë‹µë³€ ê¸¸ì´ ì œí•œ
        )

        # LLM ì‘ë‹µì—ì„œ ë‹µë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        answer = response.choices[0].message.content.strip()

        # ê²°ê³¼ êµ¬ì„±
        result = {
            "answer": answer,
            "metadata": {
                "model": self.model,
                "num_sources": len(search_results),
                "search_success": True,
                "total_tokens": response.usage.total_tokens,
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens
            }
        }

        # ì¶œì²˜ ë¬¸ì„œ í¬í•¨ ì—¬ë¶€
        if return_sources:
            result["sources"] = [
                {
                    "text": doc["text"],
                    "category": doc["metadata"].get("category", "N/A"),
                    "source": doc["metadata"].get("source", "N/A"),
                    "score": doc["score"],
                    "distance": doc["distance"]
                }
                for doc in search_results
            ]

        return result

    def batch_query(self, questions: List[str]) -> List[Dict[str, any]]:
        """
        ë°°ì¹˜ ì§ˆì˜ì‘ë‹µ

        ì—¬ëŸ¬ ì§ˆë¬¸ì„ í•œ ë²ˆì— ì²˜ë¦¬ (í‰ê°€ìš©)

        Args:
            questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            List[Dict]: ê° ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ê²°ê³¼
        """
        return [self.query(q) for q in questions]

    def get_stats(self) -> Dict[str, any]:
        """
        RAG ì‹œìŠ¤í…œ í†µê³„

        Returns:
            Dict: ì‹œìŠ¤í…œ ì„¤ì • ë° í†µê³„
        """
        return {
            "model": self.model,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "top_k": self.top_k,
            "vector_store_stats": self.vector_store.get_stats()
        }


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    import os
    from dotenv import load_dotenv

    load_dotenv()

    # ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ (ì‚¬ì „ì— êµ¬ì¶•ë˜ì–´ ìˆì–´ì•¼ í•¨)
    vector_store = FAISSVectorStore()

    try:
        vector_store.load("./vector_db")
        print("âœ… ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì™„ë£Œ")
    except FileNotFoundError:
        print("âŒ ë²¡í„° ì €ì¥ì†Œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € build_vector_db.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        exit(1)

    # RAG ì—”ì§„ ì´ˆê¸°í™”
    engine = RAGQueryEngine(
        vector_store=vector_store,
        model=os.getenv("RAG_LLM_MODEL", "gpt-4-turbo-preview"),
        temperature=float(os.getenv("RAG_TEMPERATURE", "0.3")),
        max_tokens=int(os.getenv("RAG_MAX_TOKENS", "500")),
        top_k=int(os.getenv("RAG_TOP_K", "3"))
    )

    print(f"\nğŸ“Š RAG ì‹œìŠ¤í…œ í†µê³„:")
    print(engine.get_stats())

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_questions = [
        "í—¬ë©§ ì•ˆ ì“°ë©´ ë²Œê¸ˆ ì–¼ë§ˆì•¼?",
        "í—¬ë©§ ì˜¬ë°”ë¥´ê²Œ ì°©ìš©í•˜ëŠ” ë°©ë²• ì•Œë ¤ì¤˜",
        "ì „ë™í‚¥ë³´ë“œ íƒ€ë‹¤ê°€ ì‚¬ê³ ë‚˜ë©´ ì–´ë–»ê²Œ í•´?",
    ]

    print("\n" + "="*80)
    print("RAG ì§ˆì˜ì‘ë‹µ í…ŒìŠ¤íŠ¸")
    print("="*80)

    for question in test_questions:
        print(f"\nâ“ ì§ˆë¬¸: {question}")
        result = engine.query(question)

        print(f"ğŸ’¬ ë‹µë³€: {result['answer']}")
        print(f"ğŸ“š ì°¸ê³  ë¬¸ì„œ ìˆ˜: {result['metadata']['num_sources']}")
        print(f"ğŸ”¢ í† í° ì‚¬ìš©: {result['metadata']['total_tokens']} tokens")

        if result.get("sources"):
            print("\nğŸ“– ì¶œì²˜:")
            for i, source in enumerate(result["sources"], 1):
                print(f"  [{i}] {source['source']} (ìœ ì‚¬ë„: {source['score']:.3f})")
