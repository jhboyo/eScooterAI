"""
Vector Database Builder

ì „ë™í‚¥ë³´ë“œ ì•ˆì „ ë¬¸ì„œë¥¼ FAISS ë²¡í„° DBë¡œ êµ¬ì¶•
ë²•ê·œ, ê°€ì´ë“œ, ì‚¬ë¡€ ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œ ìƒì„±
"""

import os
import json
from pathlib import Path
from typing import List, Dict
from dotenv import load_dotenv

from .vector_store import FAISSVectorStore


def load_documents_from_json(json_path: str) -> List[Dict[str, any]]:
    """
    JSON íŒŒì¼ì—ì„œ ë¬¸ì„œ ë¡œë“œ

    Args:
        json_path: JSON íŒŒì¼ ê²½ë¡œ

    Returns:
        List[Dict]: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (text, metadata)
    """
    with open(json_path, "r", encoding="utf-8") as f:
        documents = json.load(f)

    print(f"âœ… Loaded {len(documents)} documents from {json_path}")
    return documents


def build_vector_database(
    docs_dir: str,
    output_dir: str,
    embedding_dimension: int = 1536,
    api_key: str = None
) -> FAISSVectorStore:
    """
    ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•

    1. ë²•ê·œ, ê°€ì´ë“œ, ì‚¬ë¡€ JSON íŒŒì¼ ë¡œë“œ
    2. ê° ë¬¸ì„œë¥¼ OpenAI ì„ë² ë”©ìœ¼ë¡œ ë²¡í„°í™”
    3. FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
    4. ë””ìŠ¤í¬ì— ì €ì¥

    Args:
        docs_dir: ë¬¸ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        output_dir: ë²¡í„° DB ì €ì¥ ê²½ë¡œ
        embedding_dimension: ì„ë² ë”© ì°¨ì› (ê¸°ë³¸: 1536)
        api_key: OpenAI API í‚¤

    Returns:
        FAISSVectorStore: êµ¬ì¶•ëœ ë²¡í„° ì €ì¥ì†Œ
    """
    print("\n" + "="*80)
    print("ğŸ“š ì „ë™í‚¥ë³´ë“œ ì•ˆì „ êµìœ¡ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•")
    print("="*80 + "\n")

    # ========================================================================
    # 1. ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
    # ========================================================================
    # FAISS IndexFlatL2 ê¸°ë°˜ ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    # - embedding_dimension: OpenAI text-embedding-3-smallì˜ ì°¨ì› (1536)
    # - L2 ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ ì¤€ë¹„
    print(f"ğŸ”§ ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” (ì°¨ì›: {embedding_dimension})")
    vector_store = FAISSVectorStore(dimension=embedding_dimension, api_key=api_key)

    # ========================================================================
    # 2. ë¬¸ì„œ ë¡œë“œ (Knowledge Base êµ¬ì¶•)
    # ========================================================================
    # ì•ˆì „ êµìœ¡ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ 3ê°œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ì—¬ ë¡œë“œ
    # - laws.json: ë²•ê·œ ë¬¸ì„œ (ë„ë¡œêµí†µë²•, ê³¼íƒœë£Œ ë“±)
    # - guides.json: ì•ˆì „ ê°€ì´ë“œ (í—¬ë©§ ì°©ìš©ë²•, ìš´ì „ ìˆ˜ì¹™ ë“±)
    # - cases.json: ì‚¬ê³  ì‚¬ë¡€ ë° í†µê³„
    docs_path = Path(docs_dir)
    all_documents = []  # ì „ì²´ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•©

    # ë²•ê·œ ë¬¸ì„œ ë¡œë“œ
    laws_path = docs_path / "laws.json"
    if laws_path.exists():
        laws_docs = load_documents_from_json(str(laws_path))
        all_documents.extend(laws_docs)  # ì „ì²´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    else:
        print(f"âš ï¸  Laws file not found: {laws_path}")

    # ê°€ì´ë“œ ë¬¸ì„œ ë¡œë“œ
    guides_path = docs_path / "guides.json"
    if guides_path.exists():
        guides_docs = load_documents_from_json(str(guides_path))
        all_documents.extend(guides_docs)
    else:
        print(f"âš ï¸  Guides file not found: {guides_path}")

    # ì‚¬ë¡€ ë¬¸ì„œ ë¡œë“œ
    cases_path = docs_path / "cases.json"
    if cases_path.exists():
        cases_docs = load_documents_from_json(str(cases_path))
        all_documents.extend(cases_docs)
    else:
        print(f"âš ï¸  Cases file not found: {cases_path}")

    if not all_documents:
        raise ValueError("No documents found. Please check the documents directory.")

    print(f"\nğŸ“Š Total documents: {len(all_documents)}")

    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    category_stats = {}
    for doc in all_documents:
        category = doc["metadata"].get("category", "Unknown")
        category_stats[category] = category_stats.get(category, 0) + 1

    print("\nğŸ“ˆ Category Statistics:")
    for category, count in category_stats.items():
        print(f"  - {category}: {count} documents")

    # ========================================================================
    # 3. ì„ë² ë”© ë° FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
    # ========================================================================
    # ê° ë¬¸ì„œë¥¼ OpenAI APIë¡œ ì„ë² ë”©í•˜ê³  FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
    # - í…ìŠ¤íŠ¸ â†’ 1536ì°¨ì› ë²¡í„° ë³€í™˜ (ì˜ë¯¸ì  í‘œí˜„)
    # - FAISS IndexFlatL2ì— ë²¡í„° ì €ì¥
    # - ì‹œê°„ ë³µì¡ë„: O(N * D) where N=ë¬¸ì„œ ìˆ˜, D=ì°¨ì› ìˆ˜
    # - API í˜¸ì¶œ: Në²ˆ (ë¬¸ì„œë‹¹ 1ë²ˆ)
    print(f"\nğŸ”„ Embedding documents with OpenAI text-embedding-3-small...")
    print("   (This may take a few minutes depending on the number of documents)")

    vector_store.add_documents(all_documents)

    print(f"âœ… Successfully embedded {len(all_documents)} documents")

    # ========================================================================
    # 4. ë²¡í„° ì €ì¥ì†Œ ë””ìŠ¤í¬ì— ì €ì¥
    # ========================================================================
    # FAISS ì¸ë±ìŠ¤ì™€ ë¬¸ì„œ ë°ì´í„°ë¥¼ ì˜êµ¬ ì €ì¥
    # - {output_dir}/index.faiss: FAISS ë²¡í„° ì¸ë±ìŠ¤ (ë°”ì´ë„ˆë¦¬)
    # - {output_dir}/documents.json: ì›ë³¸ í…ìŠ¤íŠ¸ + ë©”íƒ€ë°ì´í„° (JSON)
    # ëŸ°íƒ€ì„ì— load() ë©”ì„œë“œë¡œ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš© ê°€ëŠ¥
    print(f"\nğŸ’¾ Saving vector database to {output_dir}")
    vector_store.save(output_dir)

    print("\nâœ… Vector database built successfully!")
    print(f"\nğŸ“Š Final Statistics:")
    stats = vector_store.get_stats()
    for key, value in stats.items():
        print(f"  - {key}: {value}")

    return vector_store


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()

    # ì„¤ì •
    PROJECT_ROOT = os.getenv("PROJECT_ROOT", os.getcwd())
    DOCS_DIR = os.path.join(PROJECT_ROOT, "src/data/safety_docs")
    OUTPUT_DIR = os.getenv("VECTOR_DB_PATH", "./vector_db")
    EMBEDDING_DIMENSION = int(os.getenv("EMBEDDING_DIMENSION", "1536"))
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    if not OPENAI_API_KEY:
        print("âŒ Error: OPENAI_API_KEY not found in .env file")
        print("   Please set your OpenAI API key in .env:")
        print("   OPENAI_API_KEY=sk-your-api-key-here")
        return

    # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
    try:
        vector_store = build_vector_database(
            docs_dir=DOCS_DIR,
            output_dir=OUTPUT_DIR,
            embedding_dimension=EMBEDDING_DIMENSION,
            api_key=OPENAI_API_KEY
        )

        # ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\n" + "="*80)
        print("ğŸ§ª Quick Search Test")
        print("="*80 + "\n")

        test_queries = [
            "í—¬ë©§ ì•ˆ ì“°ë©´ ë²Œê¸ˆ?",
            "í—¬ë©§ ì°©ìš©ë²•",
            "ì‚¬ê³  ì‚¬ë¡€"
        ]

        for query in test_queries:
            print(f"\nğŸ” Query: {query}")
            results = vector_store.search(query, top_k=2)

            for i, result in enumerate(results, 1):
                print(f"\n  [{i}] Score: {result['score']:.3f}, Distance: {result['distance']:.3f}")
                print(f"      Text: {result['text'][:100]}...")
                print(f"      Category: {result['metadata'].get('category', 'N/A')}")

        print("\n" + "="*80)
        print("âœ… Vector database is ready for use!")
        print("   Run 'uv run python src/rag/query_engine.py' to test the RAG system")
        print("="*80)

    except Exception as e:
        print(f"\nâŒ Error building vector database: {e}")
        raise


if __name__ == "__main__":
    main()
